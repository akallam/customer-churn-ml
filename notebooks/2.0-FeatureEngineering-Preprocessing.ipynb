{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ce9a59-3773-4531-877b-b54b2dab9749",
   "metadata": {},
   "source": [
    "# Telco Customer Churn Prediction - Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2df5c-a2b4-41f2-95fe-c716d3715454",
   "metadata": {},
   "source": [
    "## 1.0 Introduction\n",
    "\n",
    "This notebook extends the work from `1.0-EDA-DataCleaning.ipynb` by focusing on preparing the cleaned Telco Churn dataset for machine learning model training. This involves transforming categorical features into a numerical format, scaling numerical features, and splitting the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38162bb-399b-4461-81d8-adae62dec17d",
   "metadata": {},
   "source": [
    "## 2.0 Data Loading and Initial Setup\n",
    "* **Objective:** Load the dataset and re-apply the essential cleaning and initial preprocessing steps from the previous notebook to ensure the data is in a consistent state before advanced preprocessing.\n",
    "### 2.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1929c70-0485-46f4-a842-9332cbb91bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ab736-8d5f-4285-8a14-bc9b3c3c9352",
   "metadata": {},
   "source": [
    "### 2.2 Load Data and Re-apply Basic Cleaning\n",
    "The data is loaded, and the essential cleaning steps (column renaming, totalcharges handling, churn conversion, and customerid removal) are reapplied to get the DataFrame into its cleaned state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7bc73b8-8ce0-401e-954e-148779316854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load raw data\n",
    "df = pd.read_csv('../data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "#perform cleaning/preprocessing steps from EDA\n",
    "df.columns = df.columns.str.lower()\n",
    "df.drop('customerid', axis=1, inplace=True)\n",
    "df['totalcharges'] = pd.to_numeric(df['totalcharges'], errors='coerce')\n",
    "df.dropna(subset=['totalcharges'], inplace=True)\n",
    "df['churn'] = df['churn'].map({'Yes': 1, 'No':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a63b9929-9282-4af7-95b0-d71077b6d048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7032, 20)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ec59f60-5251-4c3e-8caa-d9457b6f53d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7032 entries, 0 to 7042\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   gender            7032 non-null   object \n",
      " 1   seniorcitizen     7032 non-null   int64  \n",
      " 2   partner           7032 non-null   object \n",
      " 3   dependents        7032 non-null   object \n",
      " 4   tenure            7032 non-null   int64  \n",
      " 5   phoneservice      7032 non-null   object \n",
      " 6   multiplelines     7032 non-null   object \n",
      " 7   internetservice   7032 non-null   object \n",
      " 8   onlinesecurity    7032 non-null   object \n",
      " 9   onlinebackup      7032 non-null   object \n",
      " 10  deviceprotection  7032 non-null   object \n",
      " 11  techsupport       7032 non-null   object \n",
      " 12  streamingtv       7032 non-null   object \n",
      " 13  streamingmovies   7032 non-null   object \n",
      " 14  contract          7032 non-null   object \n",
      " 15  paperlessbilling  7032 non-null   object \n",
      " 16  paymentmethod     7032 non-null   object \n",
      " 17  monthlycharges    7032 non-null   float64\n",
      " 18  totalcharges      7032 non-null   float64\n",
      " 19  churn             7032 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(15)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6af1b2-0d1b-40c8-b36a-4305dd8a6c3f",
   "metadata": {},
   "source": [
    "## 3.0 Feature and Target Separation\n",
    "* **Objective:** Separate the independent features X from the dependent feature (target, y). This is a standard practice before applying machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1705288c-da20-41ee-99b6-bb31e5405268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (7032, 19)\n",
      "y shape: (7032,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('churn', axis=1)\n",
    "y = df['churn']\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5c3b9b-391b-4d0b-bce1-9a71d178286e",
   "metadata": {},
   "source": [
    "## 4.0 Feature Engineering and Preprocessing Pipelines\n",
    "* **Objective:** Prepare the features for machine learning models. This involves converting categorical features into a numerical format and scaling numerical features.\n",
    "### 4.1 Identify column types\n",
    "* Features are categorized into numerical and categorical types to apply appropriate preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b7275c9-ceeb-4711-bd3c-e1d488cac661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split features into numerical and categorical features for preprocessing steps. \n",
    "numerical_features = ['tenure', 'monthlycharges', 'totalcharges']\n",
    "categorical_features = [col for col in X.columns if col not in numerical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2a813-2fbd-4753-9e4c-de57ba503119",
   "metadata": {},
   "source": [
    "### 4.2 One-Hot Encoding for Categorical Features\n",
    "* **Implementation:** We use `OneHotEncoder` from `sklearn.preprocessing`. The `handle_unknown='ignore'` parameter prevents errors if a new, unseen category appears during testing. The `drop='first'` parameter is used to prevent multicollinearity (the \"Dummy Variable Trap\") by dropping one of the one-hot encoded columns for each original feature.\n",
    "### 4.3 Scaling for Numerical Categories\n",
    "* **Implementation:** `StandardScaler` from `sklearn.preprocessing` is applied to the numerical features.\n",
    "### 4.4 Combining Preprocessing Steps with ColumnTransformer\n",
    "* **Implementation:** `ColumnTransformer` is a powerful tool from `sklearn.compose` that allows different transformations to be applied to different columns of the input data simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dded370e-982f-47ec-a1a0-783cc8347028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_features), \n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
    "     ])\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab5367-de01-4858-b090-64933fcabb56",
   "metadata": {},
   "source": [
    "## 5.0 Train-Test Split\n",
    "* **Objective:** Divide the processed dataset into training and testing subsets. The model will be trained solely on the training data and evaluated on the unseen testing data to assess its generalization performance.\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- `test_size=0.2`: 20% of the data will be used for the test set, and 80% for the training set.\n",
    "- `random_state=42`: A fixed integer for the random state ensures that the split is reproducible, meaning you'll get the same train/test split every time you run the code.\n",
    "- `stratify=y`: This is crucial for classification tasks, especially with imbalanced datasets (like churn). It ensures that the proportion of target classes (churned vs. non-churned) is approximately the same in both the training and testing sets as it is in the original dataset. This prevents scenarios where one set might have significantly more or fewer churned customers, leading to biased evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7038b498-9f65-4831-b0bd-4d2f325dce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, random_state=42, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5423cde8-b5a9-4b79-bc8c-b1c74ff1b4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5625, 30)\n",
      "X_test shape: (1407, 30)\n",
      "y_train shape: (5625,)\n",
      "y_test shape: (1407,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67472480-cc05-42e2-8271-99c92a4c3c07",
   "metadata": {},
   "source": [
    "## 6.0 Saving Processed Data\n",
    "* **Objective:** Save the preprocessed and split datasets (X_train, X_test, y_train, y_test) to disk. This allows for quick loading in subsequent notebooks (e.g., for model training) without re-running the entire preprocessing pipeline.\n",
    "\n",
    "* **Method:** The data is saved using numpy.savez, which efficiently stores multiple NumPy arrays in a single compressed .npz file. A dedicated `data/processed/` directory is used for these output files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5dfc9d9a-07c7-45a6-bbb1-2447983c23c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data saved to ../data/processed/telco_churn_processed_data.npz\n"
     ]
    }
   ],
   "source": [
    "processed_data_dir = '../data/processed/'\n",
    "np.savez(processed_data_dir+'telco_churn_processed_data.npz', \n",
    "         X_train=X_train, \n",
    "         X_test=X_test,\n",
    "         y_train=y_train,\n",
    "         y_test=y_test)\n",
    "print(f\"\\nProcessed data saved to {processed_data_dir}telco_churn_processed_data.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb985d7-c99f-4c8a-b207-46e8679e0466",
   "metadata": {},
   "source": [
    "## 7.0 Conclusion & Next Steps\n",
    "This notebook has successfully transformed the raw Telco Churn dataset into a clean, preprocessed, and split format suitable for machine learning.\n",
    "\n",
    "The next steps will involve:\n",
    "1. **Model Training:** Selecting and training various machine learning classification models.\n",
    "2. **Model Evaluation:** Assessing model performance using appropriate metrics (e.g., accuracy, precision, recall, F1-score, ROC AUC).\n",
    "3. **Hyperparameter Tuning:** Optimizing model parameters for improved performance.\n",
    "4. **Model Interpretation:** Gaining insights into which features are most important for predicting churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524d9b6-5dfe-468d-8d21-422ec2edab6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
